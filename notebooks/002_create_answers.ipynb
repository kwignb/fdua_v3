{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/kwatanabe/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/kwatanabe/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download(\"averaged_perceptron_tagger_eng\")\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import tiktoken\n",
    "from glob import glob\n",
    "from tqdm.auto import tqdm\n",
    "from time import sleep\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from src.dataset.postprocess import process_markdown_file  # noqa: E402\n",
    "from src.tools.create_docs import process_files_in_batches  # noqa: E402\n",
    "from src.model.retriever import create_retriever  # noqa: E402\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markdownファイルについて後処理を実施\n",
    "- 必要のない文章を含む行を指定して削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_md_paths_bef = sorted(glob(\"../data/documents/gpt_4omini_markdowns/*.md\"))\n",
    "for val_md_path in val_md_paths_bef:\n",
    "    process_markdown_file(\n",
    "        input_file=val_md_path,\n",
    "        line_target_words=[\"統合報告書\", \"統合レポート\"],\n",
    "        header_keywords=[\"INDEX\", \"目次\"],\n",
    "        output_dir=\"../data/documents/gpt_4omini_markdowns/postprocess\"\n",
    "    )\n",
    "\n",
    "test_md_paths_bef = sorted(glob(\"../data/test/documents/markdowns/*.md\"))\n",
    "for test_md_path in test_md_paths_bef:\n",
    "    process_markdown_file(\n",
    "        input_file=test_md_path,\n",
    "        line_target_words=[\"統合報告書\", \"統合レポート\"],\n",
    "        header_keywords=[\"INDEX\", \"目次\"],\n",
    "        output_dir=\"../data/test/documents/markdowns/postprocess\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_query = pl.read_csv(\"../signate_data/validation/ans_txt.csv\")\n",
    "val_md_paths = sorted(glob(\"../data/documents/gpt_4omini_markdowns/postprocess/*.md\"))\n",
    "test_query = pl.read_csv(\"../signate_data/query.csv\")\n",
    "test_md_paths = sorted(glob(\"../data/test/documents/markdowns/postprocess/*.md\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector DBを作成\n",
    "- バッチ処理による文書のVector DB格納作業を実施"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    model=os.getenv(\"EMBEDDING\")\n",
    ")\n",
    "\n",
    "target_words = [\"統合報告書\", \"統合レポート\"]\n",
    "header_words = [\"INDEX\", \"目次\"]\n",
    "chunk_size = 500\n",
    "chunk_overlap = 0\n",
    "\n",
    "val_vector_store = process_files_in_batches(\n",
    "    embeddings=embeddings,\n",
    "    md_paths=val_md_paths,\n",
    "    target_words=target_words,\n",
    "    header_words=header_words,\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap\n",
    "    )\n",
    "\n",
    "test_vector_store = process_files_in_batches(\n",
    "    embeddings=embeddings,\n",
    "    md_paths=test_md_paths,\n",
    "    target_words=target_words,\n",
    "    header_words=header_words,\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieverの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_config = {\n",
    "    \"topk\": 30,\n",
    "    \"hybrid\": True,\n",
    "    \"hybrid_topk\": 30,\n",
    "    \"hybrid_weights\": [0.5, 0.5],\n",
    "    \"rerank\": False,\n",
    "    \"rerank_topk\": 10\n",
    "}\n",
    "\n",
    "val_retriever = create_retriever(\n",
    "    vector_store=val_vector_store,\n",
    "    topk=retriever_config[\"topk\"],\n",
    "    hybrid=retriever_config[\"hybrid\"],\n",
    "    hybrid_topk=retriever_config[\"hybrid_topk\"],\n",
    "    hybrid_weights=retriever_config[\"hybrid_weights\"],\n",
    "    rerank=retriever_config[\"rerank\"],\n",
    "    rerank_topk=retriever_config[\"rerank_topk\"]\n",
    ")\n",
    "\n",
    "test_retriever = create_retriever(\n",
    "    vector_store=test_vector_store,\n",
    "    topk=retriever_config[\"topk\"],\n",
    "    hybrid=retriever_config[\"hybrid\"],\n",
    "    hybrid_topk=retriever_config[\"hybrid_topk\"],\n",
    "    hybrid_weights=retriever_config[\"hybrid_weights\"],\n",
    "    rerank=retriever_config[\"rerank\"],\n",
    "    rerank_topk=retriever_config[\"rerank_topk\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Promptの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_schemas = [\n",
    "    ResponseSchema(\n",
    "        name=\"answer\",\n",
    "        description=\"質問に対しその回答となる要素のみ（前後の文章は一切不要）を端的に出力する\"\n",
    "        )\n",
    "]\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "\n",
    "qa_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=(\"\"\"\n",
    "        あなたは、簡潔で正確な回答を生成するAIアシスタントです。以下のルールを**絶対に遵守**してください。\n",
    "\n",
    "\n",
    "        ### **⚠ 絶対に守るべきルール**\n",
    "        - 質問内に特定の指示（例：「四捨五入して答えよ」「株式会社をつけて」など）がある場合、**その指示に従って回答する**\n",
    "        - 回答は必ず **54トークン以内** で出力する\n",
    "        - 必要があれば読点を用いて横並びに出力する\n",
    "        - **回答に数値を含む場合は、単位を明記** すること（例：「36拠点」、「3.0％」）\n",
    "        - **質問に対して直接的な回答のみを出力する**（「以下が回答です：」のような前置きは不要）\n",
    "        - 参照する情報に不備があり、回答に確証が得られない場合は必ず「不明」と出力する\n",
    "\n",
    "        ### ** 入力例 **\n",
    "        質問: A社の収益率は2018年度と2019年度ではどちらの数値が高いか\n",
    "        回答: 2018年度\n",
    "\n",
    "        質問: C社の支店数は何支店ですか？\n",
    "        回答: 39支店\n",
    "\n",
    "        質問: D社の2019年度の利益率は2018年度に比べて何%向上したか、少数第二位を四捨五入して答えよ\n",
    "        回答: 21.2%\n",
    "\n",
    "        質問: E社の2024年度の売上高は何億円になると予測できますか？\n",
    "        回答: 83億円\n",
    "\n",
    "        出力フォーマット: {format_instructions}\n",
    "        情報: {context}\n",
    "        質問: {question}\n",
    "        回答:\n",
    "    \"\"\"\n",
    "    ),\n",
    "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "def setup_qa_chain(client, retriever, prompt):\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=client,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        chain_type_kwargs={\"prompt\": prompt},\n",
    "        return_source_documents=True\n",
    "    )\n",
    "    return qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AzureChatOpenAI(\n",
    "    openai_api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    deployment_name=os.getenv(\"MODEL\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    temperature=0,\n",
    "    top_p=1,\n",
    "    max_tokens=54,\n",
    ")\n",
    "\n",
    "val_qa_chain = setup_qa_chain(client, val_retriever, qa_prompt)\n",
    "test_qa_chain = setup_qa_chain(client, test_retriever, qa_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_answers(query, qa_chain, max_retries=3):\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "    indices = [i for i in range(len(query))]\n",
    "    answers = []\n",
    "    source_documents = []\n",
    "    for i, p in tqdm(enumerate(query[\"problem\"]), total=len(query)):\n",
    "        retries = 0\n",
    "        while retries < max_retries:\n",
    "            try:\n",
    "                qa = qa_chain.invoke({\"query\": p})\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Attempt {retries+1} failed with error: {e}\")\n",
    "                retries += 1\n",
    "                sleep(2 ** retries)\n",
    "        else:\n",
    "            print(f\"Failed to process problem {i} after {max_retries} attempts.\")\n",
    "        try:\n",
    "            answer = output_parser.parse(qa[\"result\"])[\"answer\"]\n",
    "            token = encoding.encode(answer)\n",
    "            if len(token) > 54:\n",
    "                answer = \"不明\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing response for query {i}: {e}\")\n",
    "            answer = \"Error\"\n",
    "        print(answer)\n",
    "        answers.append(answer)\n",
    "        source_documents.append(qa[\"source_documents\"])\n",
    "\n",
    "    df = pl.DataFrame(\n",
    "        data={\n",
    "            \"index\": indices,\n",
    "            \"answer\": answers\n",
    "        },\n",
    "        schema={\n",
    "            \"index\": pl.UInt32,\n",
    "            \"answer\": pl.String\n",
    "        }\n",
    "    )\n",
    "    return df, source_documents\n",
    "\n",
    "def save_csv(df, output_path):\n",
    "    csv_data = df.write_csv().split(\"\\n\", 1)[-1]\n",
    "    with open(output_path, \"w\") as f:\n",
    "        f.write(csv_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 検証データにおける回答生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df, val_sd = create_answers(query=val_query, qa_chain=val_qa_chain)\n",
    "save_csv(df=val_df, output_path=\"../signate_data/evaluation/submit/predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 検証データのスコア計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../signate_data/evaluation/crag.py \\\n",
    "    --result-dir ../signate_data/evaluation/submit \\\n",
    "    --ans-dir ../signate_data/evaluation/data \\\n",
    "    --eval-result-dir ../signate_data/evaluation/result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## テストデータにおける回答生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df, test_sd = create_answers(query=test_query, qa_chain=test_qa_chain)\n",
    "save_csv(df=test_df, output_path=\"../data/test/submit/predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
